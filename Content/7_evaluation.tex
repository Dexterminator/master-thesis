\section{Parallel program profiler analysis}
After the parallel program implementation, another round of profiling using \code{cProfile} was conducted. This was done on the initial testing laptop,
with 4 workers, on the same dataset as the first round of profiling. The results from the main process that orchestrates the other processes and aggregates their results can be
found in \ref{fig:parallel_profiler_main}. The results from two of the workers can be found in figure \ref{fig:parallel_profiler_w1} and figure \ref{fig:parallel_profiler_w2}.

For the main process, it is apparent that \code{post\_process\_parallel}, the function that makes the trade ID:s unique after the parallel worker processes have finished
executing, adds non-negligible overhead. \code{aggregate\_result} contains the code that creates the other processes and waits for these to produce their partial results,
aggregating them as they are produced. This code takes up 84\% of the total time, while the remaining 16\% of the code is effectively overhead.
In addtion, as mentioned in section \ref{section:sequential_profiler} as a possibility, \code{\_prepare} is called for each
worker process, resulting in extra overhead. It is conceivable that these sources of overhead are less noticable when processing a larger dataset, as the effects of
the functions that are called only one time take up a smaller amount of the total running time if the dataset contains a larger number of rows.

For the worker processes, the results are similar to the results from profiling the sequential program in section \ref{section:sequential_profiler}, but with lower
\code{cumtime} values. This is expected due to the lower number of rows per worker. The same functions as in the sequential program are responsible for the largest
portion of the running time, \code{process\_record}, \code{post\_process\_record}, \code{consume\_record}, and \code{\_prepare}.

\begin{figure}[ht]
  \lstinputlisting[basicstyle=\tiny]{figures/profiling_parallel_19_may_main.txt}
  \caption{Main process in parallel program \code{cProfile} output}
  \label{fig:parallel_profiler_main}
\end{figure}
\clearpage

\begin{figure}[ht]
  \lstinputlisting[basicstyle=\tiny]{figures/profiling_parallel_19_may_31147.txt}
  \caption{Worker 1 in parallel program \code{cProfile} output}
  \label{fig:parallel_profiler_w1}
\end{figure}
\clearpage

\begin{figure}[ht]
  \lstinputlisting[basicstyle=\tiny]{figures/profiling_parallel_19_may_31148.txt}
  \caption{Worker 2 in parallel program \code{cProfile} output}
  \label{fig:parallel_profiler_w2}
\end{figure}
\clearpage

\section{Performance model calculations}

\subsection{Amdahl's law}

\subsection{Gustafson's law}

\subsection{Work-span model}


\section{Test datasets}
For all datasets, the number of filters is limited to a few more than the number of columns, as the majority of the filters are column mapping filters.
This is typical for most datasets, making this a suitable sample of datasets. No datasets with inherently serial file formats where used, as these cannot be
parllelized and would provide no interesting data. Instead, the focus of the experiments are on the \textit{Extra overhead} file formats, since these are the
most common (since making trade ID:s unique is a useful feature). They also give the fairest indication of how successful the parallelization is, as they are
both the worst case (apart from inherently serial file formats) and the common case. Another reason why this set of datasets was chosen is their differing sizes,
with potentially differing parallelization benefits.

The characteristics of the test datasets are outlined below.

\subsection{Dataset 1}
\begin{itemize}
  \item \textbf{Rows:} 102
  \item \textbf{Columns:} 46
  \item \textbf{File format family:} Extra overhead
\end{itemize}

\subsection{Dataset 2}
\begin{itemize}
  \item \textbf{Rows:} 2,890
  \item \textbf{Columns:} 46
  \item \textbf{File format family:} Extra overhead
\end{itemize}

\subsection{Dataset 3}
\begin{itemize}
  \item \textbf{Rows:} 23,763
  \item \textbf{Columns:} 46
  \item \textbf{File format family:} Extra overhead
\end{itemize}

\subsection{Dataset 4}
\begin{itemize}
  \item \textbf{Rows:} 338,730
  \item \textbf{Columns:} 89
  \item \textbf{File format family:} Extra overhead
\end{itemize}

\subsection{Experiments}
The experiments where conducted on the CentOS computer with 8 cores described in the Method \& Materials section.
For each dataset, experiments where run to gather information about time and memory consumption for different configurations.
The experiments entailed performing a dataset transformation using the parallel program with each number of workers in the range 1 to 2 times the number of cores: 1--16.
In addition to running the parallel program, a serial transformation was also performed as a reference point.
In each of the experiments, the Python \code{resource} module was used in each worker process to find the user time, system time, and maximum memory consumption.
These values where then summed in order to find the total resource use. In the case of maximum memory consumption, this means that the number found is a worst case
number. It is possible that the processes in total have a smaller maximum memory consumption since they may reach their maximum memory consumption at different
points in time.

In order to provide more accurate results, the experiments where run 10 times for each dataset--worker number configuration. These 10 runs where then used to calculate the
average value as well as the standard deviation for each metric value. In addition, the speedup was calculated by dividing the sequential execution time with the parallel execution
time for each worker value.
