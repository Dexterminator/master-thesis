The results in the previous chapter are discussed below, in an effort to explain them. In addition, final conclusions
about the thesis as a whole are drawn.

\section{Dataset benchmarks discussion}
\subsection{Tiny dataset discussion}
The tiny dataset shows poor results when parallelizing. For every worker number, only slowdown can be observed.
The real time values for 10 and 12 workers show high standard deviations when comparing with the average value. It is conceivable that
tasks such as creating processes have a more noticeable impact for the low execution time.

The profiling session for the main process for the tiny dataset showed that \code{aggregate\_result} takes up 70\% of the time. Since this is the function
that performs the main parallel processing of the dataset, about 30\% of the execution time is overhead due to for example header detection
and making trade ID:s unique. Since \code{aggregate\_result} takes 0.4 seconds (15.7\% of execution time ) more to execute than it takes for a single worker to finish,
it appears as though parallelization and aggregating the partial results result in noticeable overhead for the tiny dataset.
It is conceivable that the above results in the fact that only slowdown can be observed when parallelizing the tiny dataset.

Even without the parallel aggregation step, the tiny dataset still results in slowdown. A steady, close to linear increase in memory usage can be observed,
resulting in total memory usage several times above what is used for the sequential program.

\subsection{Small dataset discussion}
For the small dataset, some speedup scaling can be observed, with the maximum value around 2.1. As more workers are added, speedup is increased,
flattening out around 8 workers. This fits with the fact that the testing machine has 8 cores, effectively making it able to utilize 
true parallelism for a maximum of 8 workers. The impact of adding more workers decreases with each one that is added. In the profiling session
\code{post\_process\_parallel} is shown to take up a relatively large portion of the execution time (22.5\%). Since this function is a separate,
sequential and constant step, it is not affected by parallelization and therefore takes up varying percentages of the total time. Since more parallelization
leads to faster execution of the parallel portion of the code, the post processing becomes more significant as workers are added, contributing to the fact
that adding more workers results in less and less increase in speedup.

Without the post processing step, the small dataset gets slightly better speedup for 8 workers, 2.7X instead of 2.11X. This is still not close to
the performance model calculations, which can be seen as disappointing. The memory usage grows in a similar manner to the tiny dataset, though the values are greater.
Real time shows relatively low standard deviations compared to the total time, indicating that real time is fairly accurate for each worker
value.

\subsection{Medium dataset discussion}
The medium dataset shows greater speedup than the small dataset, with a similar shape to the speedup by worker curve. The maximum speedup is
around 3.8, evening out around 8 workers, once again showing the best results around the machine's core number. This further suggests that larger
individual workload, this time a result of the larger dataset size, results in better speedup. Real time again shows
low standard deviation. Memory usage for the worker numbers with the largest speedup values is around 3 GB for dataset 3, demonstrating
that a large price in memory usage is paid for parallelization, which may impact performance negatively for large dataset sizes and
worker numbers.

The profiling session for the main process shows that \code{aggregate\_result} (30.138 s) takes a similar amount of time to the execution of
the worker processes (30.050 s). This suggests that parallelization overhead is not a major part of the execution time for the medium dataset.
However, in the worker process the \code{process} function (handling the main dataset processing) takes up 28.802 s of the total 30.050 s that the
worker takes to execute. This suggests a small overhead in the form of worker startup (4\%).
The post processing step takes up about 7.0\% of the execution time for 8 workers. This is less than for the small dataset, but may still contribute
to the fact that adding workers results in less increase in speedup.

Without the parallel processing, the medium dataset shows slightly better speedup for 8 workers. However, this increase is only from 3.76X to 4.04X
speedup, meaning that it still has relatively low efficiency.

\subsection{Large dataset discussion}
The large dataset shows an even greater speedup, maintaining the trend of larger parallelization gains for larger datasets. Speedup for the large
dataset increases past 8 workers, flattening out around 11 workers. From the profiling session, it is evident that workers for the larger dataset
may receive different workloads (taking between 412 seconds and 503 seconds) even though the chunks are the same size. This means that for 8 workers
(the same as the number of cores), several cores are unused as the faster workers terminate and the program waits for the slow workers to terminate.
By having more workers than cores, work can be more evenly scheduled among cores, resulting in better utilization of the processing power of the cores.

The profiling session also shows that \code{\_prepare} takes up less than 2\% of the processing time for the workers, meaning that workers for the large
dataset spend more time in the main row processing loop, likely contributing to the fact that the larger dataset shows greater speedup.

The post processing step takes up about 6.5\% of the execution time for 8 workers, close to the medium dataset. Without the parallel processing,
the large dataset shows slighlty better speedup for 8 workers. However, this increase is only from 4.3X to 4.6X speedup, meaning that it still has
relatively low efficiency.


%\subsection{Dataset 4}
%With the even larger dataset 4, another increase in speedup can be observed. Once again, the trend of greater parallelization gains for larger
%dataset sizes holds. Interestingly, speedup increases slightly even past 8 workers, though it flattens out around 12 workers.
%Though the program is largely CPU bound, the reason for the increase when using a number of workers greater than the number of cores may be
%greater CPU utilization when waiting for I/O when extra workers are added, as the chance of finding a worker with CPU bound work to do increases. 
%Real time standard deviation follows the same, stable trend also for dataset 4. Memory usage when parallelizing this significantly larger dataset,
%ranging between 5 GB and 7 GB for the worker values with the greatest speedup.

\subsection{General benchmark trends}
In general, user time increases significantly with added workers, as is expected due to the greater total CPU utilization. Together with the
fact that memory usage increases linearly with worker number, this indicates that a noteworthy amount of system resources is required 
for parallelization as dataset sizes and number of workers grow large. The highest amount of memory used by the program is 7.7 GB. While
this is a substantial amount of memory, the benchmarking machine has 32 GB of memory, meaning that the memory usage should not impact
performance in an all too significant way.

In general, larger datasets show greater speedup than smaller datasets.

\subsection{Memory usage and caching discussion}
The fact that memory usage increases as workers are added (though the overall problem size stays the same)
can be explained by the fact that the \code{multiprocessing} module creates separate, entirely
new processes. For each of these, the filters, mappings and other data relating to the transformation has to be stored in addition to the
base memory footprint of the process. 

As shown in section \ref{targeted_memory_profiling}, the mappings cached by the post process filter take up a clear majority of the memory,
and is substantial in size. The cache is in place in order to avoid many round trips to the database in order to get faster execution. However,
due to the fact that the worker processes cannot share any data when using \code{multiprocessing}, the data is duplicated across the processes,
resulting in a net increase in memory usage. For larger datasets, this increase in memory usage is very noticeable, and therefore a
significant price to pay for speedup.

\subsection{Ethics and sustainable development}
Faster processing of financial data could have a small but positive effect on society. Since faster processing facilitates faster response
cycles for the customers (banks), errors can be spotted sooner and bottlenecks in the customer's organization avoided. This can also reduce
frustration for the customer. As the banks perform services that greatly affect society, society can benefit (at least slightly) from these
positive effects of faster processing. In addition, if the datasets are processed faster through the use of multiple cores, the time until
the cores are put in the power-saving sleep mode is smaller. In today's world, where resources are depleted faster than they are replenished,
saving power is an important goal.

%As mentioned in the indidual dataset discussions, the reason for this
%is feasably that parallelization overhead and worker startup time becomes less significant as the row processing step takes up a larger portion
%of the execution time.

%Compared to the performance calculations in section \ref{performance_model_calculations} the speedup may seem disappointing. The reasons for this are likely
%manifold. The overhead of synchronization and \code{multiprocessing} process creation are not taken into account in any of the performance models,
%and may contribute to the smaller than expected speedup. Furthermore, as previously mentioned, each worker has to restart the connections to MySQL
%and Cassandra and also have to cache column mappings individually as \code{multiprocessing} does not allow for shared memory. Finally, the
%post processing step of making trade ID:s unique adds a significant sequential term to the transformation program.

\section{Conclusions}
This section outlines the final conclusions drawn from this thesis.

\subsection{Main conclusions}
This thesis shows that using Python \code{multiprocessing} for parallelization of datasets by utilizing data parallelism
and using a task-based approach is possible. However, this method was not without issues.
Sharing data using only message passing results in relatively safe and readable code since excessive sharing of data is avoided.
However, in the transformation program parallelized in this thesis, the fact that the processing pipeline including column mappings
needed to be stored for each worker resulted in more overhead both regarding worker startup and memory consumption. This
leads to the conclusion that users of multiprocessing need to be wary not only of communication and creation overhead associated
with processes (as opposed to threads), but also of overhead from worker startup and data duplication as a result of the message
passing model.

The transformation problem in this thesis, and parallel programs with expensive worker startup, are heavily influenced by the size
of the data. This means that developers faced with implementing parallelization of similar systems should examine the data in their
problem domain in order to find an initial indication of whether the size of the datasets are, in general, large enough to benefit
from parallelization.

While workload may appear to be the same when splitting a dataset into equal parts, it may still be beneficial to examine if
the workers take different times to finish, in order to avoid underused cores. Using more tasks than cores for larger datasets
can increase speedup in these cases.

When parallelizing a complex system, as few assumptions as possible should be made before starting. In this thesis, the problem
involves I/O and database communication, suggesting that the problem may be I/O bound. However, further investigations proved that the problem
was largely CPU bound, making it suitable for \code{multiprocessing} in combination with a multicore machine. Investigation, rather than
assumption, are helpful when parallelizing a complex program involving both CPU and I/O tasks.

The method used in this thesis for finding parallelizability was relatively successful. When analyzing file formats for parallelizability,
inherently serial, extra overhead, and embarrassingly parallel formats were found. The implementation then focused on parallelizing
the extra overhead and embarrassingly parallel formats. This method could possibly be generalized to other parallelization problems
in complex systems, by identifying a subsystem that is easily parallelizable and focusing on that part rather than the system as a whole.
The subsystem might be either a subset of the code or of the datasets in the problem domain. In this thesis, the subsystem proved to be
large enough that the effort of parallelization was worthwhile, which is an analysis that should be made also in the general case of
parallelizing complex systems.

Developers should be wary of aggregation when parallelizing systems. In this thesis, aggregation manifested itself as making trade ID:s unique,
which was done by storing encountered ID:s. While storing state in this manner may appear as making rows dependant on each other, it proved
possible to extract the aggregation into a post-processing step. This extra step introduced overhead, but parallel speedup could still be 
observed for larger datasets. In conclusion, implementers of parallelization should look for aggregation in their system, conclude if this
can be extracted into a post processing step, and if this post processing step is small enough for parallelization to be beneficial. Examining
if the post processing step takes up a smaller part of the total execution time for larger datasets can also be of interest.

\subsection{Delimitations}
The implementation and research in this thesis is limited to the parallelization of an existing program, and no new code for the core problem
of processing the datasets was written. Another delimitation of the thesis is that it does not compare different methods of parallelization,
and uses only the Python \code{multiprocessing} module.

\subsection{Future work}
This thesis focused on parallelization on a single shared memory system. Since \code{multiprocessing} uses a message passing approach with serialized data,
it is conceivable that a future work in a distributed approach is interesting for the dataset transformation problem for larger datasets. Also,
conducting experiments on even larger datasets may be of value as the trend points to greater speedup the larger the dataset. Another interesting
future work would be to implement a mechanism that selects a suitable parallelization strategy given the size of a dataset and its file format, with
the help of the results in this thesis.

