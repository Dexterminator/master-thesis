\section{Dataset benchmarks discussion}
\subsection{Dataset 1}
Dataset 1, with comparatively few rows, shows poor results when parallelizing. For every worker number, only slowdown can be observed.
The real time values for 10 and 12 workers show high standard deviations when comparing with the average value. It is conceivable that
tasks such as creating processes have a more noticable impact for the low execution time. The fact that parallelization results in
slowdown for the small dataset suggests that parallelization overhead in the form of creating new processes, synchronizing, and
post processing the data are large enough that any parallelization gains are shadowed.
A steady, close to linear increase in memory usage can be observed, resulting in total memory usage several times above what is
used for the sequential program. 

\subsection{Dataset 2}
For dataset 2, some speedup scaling can be observed, with the maximum value around 2.1. As more workers are added, speedup is increased,
flattening out around 8 workers. This fits with the fact that the testing machine has 8 cores, effectively making it able to utilize 
true parallelism for a maximum of 8 workers. The impact of adding more worker decreases with each one that is added, possibly suggesting
that while parallelism gains are evident, the overhead of starting up the workers become more significant as the number of workers increases
and the individual workload decreases. The memory usage grows in a similar manner to dataset 1, though the values are (as expected) greater.
Real time shows relatively low standard deviations compared to the total time, indicating that real time is fairly accurate for each worker
value.

\subsection{Dataset 3}
Dataset 3 shows greater speedup than dataset 2, with a similar shape to the speedup by worker curve. The maximum speedup is
around 3.8, evening out around 8 workers, once again showing the best results around the machine's core number. This further suggests that larger
individal workload, this time a result of the larger dataset size, makes parallelization overhead less noticable. Real time again shows
low standard deviation. Memory usage for the worker numbers with lthe largest speedup values is around 3 GB for dataset 3, demonstrating
that a large price in memory usage is paid for paralellization, which may impact performance negatively for large dataset sizes and
worker numbers.

\subsection{Dataset 4}
With the even larger dataset 4, another increase in speedup can be observed. Once again, the trend of greater parallelization gains for larger
dataset sizes holds. Interestingly, speedup increases slightly even past 8 workers, though it flattens out around 12 workers.
Though the program is largely CPU bound, the reason for the increase when using a number of workers greater than the number of cores may be
greater CPU utilization when waiting for I/O when extra workers are added, as the chance of finding a worker with CPU bound work to do increases. 
Real time standard deviation follows the same, stable trend also for dataset 4. Memory usage when parallelizing this significantly larger dataset,
ranging between 5 GB and 7 GB for the worker values with the greatest speedup.

\subsection{General benchmark trends}
In general, user time increases significantly with added workers, as is expected due to the greater total CPU utilization. Together with the
fact that memory usage increases linearly with worker number, this indicates that a noteworthy amount of system resources is required 
for parallelization as dataset sizes and number of workers grow large.

The fact that memory usage increases as workers are added (though the overall problem size stays the same)
can conceivably be explained by the fact that the \code{multiprocessing} module creates separate, entirely
new processes. For each of these, the filters, mappings and other data relating to the transformation has to be stored in addition to the
base memory footprint of the process. This means that data is duplicated across the processes, resulting in a net increase in memory usage.

In general, larger datasets show greater speedup than smaller datasets. As mentioned in the indidual dataset discussions, the reason for this
is feasably that parallelization overhead and worker startup time becomes less significant as the row processing step takes up a larger portion
of the execution time.

Compared to the performance calculations in chapter ??????????, the speedup may seem disappointing. The reasons for this are likely
manifold. The overhead of synchronization and \code{multiprocessing} process creation are not taken into account in any of the performance models,
and may contribute to the smaller than expected speedup. Furthermore, as previously mentioned, each worker has to restart the connections to MySQL
and Cassandra and also have to cache column mappings individually as \code{multiprocessing} does not allow for shared memory. Finally, the
post processing step of making trade ID:s unique adds a significant sequential term to the transformation program.

\section{Conclusions}
This section outlines the final conclusions drawn from this thesis. \\\\

Using Python \code{multiprocessing} for parallelization resulted in true parallel speedup, but was not without issues.
Sharing data using only message passing results in relatively safe and readable code since excessive sharing of data is avoided.
However, in the transformation program parallelized in this thesis, the fact that the processing pipeline including column mappings
needed to be stored for each worker resulted in more overhead both regarding worker startup and memory consumption. This
leads to the conclusion that users of multiprocessing need to be wary not only of communication and creation overhead associated
with processes (as opposed to threads), but also of overhead from worker startup and data duplication as a result of the message
passing model.

The transformation problem in this thesis, and parallel programs with expensive worker startup, are heavily influenced by the size
of the data. This means that developers faced with implemented parallelization of a similar systems should examine the data in their
problem domain in order to find an initial indication of whether the size of the datasets are, in general, large enough to benefit
from parallelization.

When parallelizing a complex system, as few assumptions as possible should be made before starting. In this thesis, the problem
involves I/O and database communication, suggesting that the problem may be I/O bound. However, further investigations proved that the problem
was largely CPU bound, making it suitable for \code{multiprocessing} in combination with a multicore machine. Investigation, rather than
assumption, are helpful when parallelizing a complex program involving both CPU and I/O tasks.

The method used in this thesis for finding parallelizability was relatively successful. When analyzing file formats for parallelizability,
inherently serial, extra overhead, and embarrassingly parallel formats where found. The implementation then focused on parallelizing
the extra overhead and embarrassingly parallel formats. This method could possibly be generalized to other parallelization problems
in complex systems, by identifying a subsystem that is easily parallelizable and focusing on that part rather than the system as a whole.
The subsystem might be either a subset of the code or of the datasets in the problem domain. In this thesis, the subsystem proved to be
large enough that the effort of parallelization was worthwhile, which is an analysis that should be made also in the general case of
parallelizing complex systems.

Developers should be wary of aggregation when parallelizing systems. In this thesis, aggregation manifested itself as making trade ID:s unique,
which was done by storing encountered ID:s. While storing state in this manner may appear as making rows dependant on each other, it proved
possible to extract the aggregation into a post-processing step. This extra step introduced overhead, but parallel speedup could still be 
observed for larger datasets. In conclusion, implementers of parallelization should look for aggregation in their system, conclude if this
can be extracted into a post processing step, and if this post processing step is small enough for parallelization to be beneficial.

