In this section, the experiments and the hardware used to conduct them are described.

\section{Hardware}
%\subsection{Initial testing laptop}
%For initial working, testing, and profiling, a laptop computer with several cores was used. The laptop is a MacBook Pro running OSX, with an Intel Core i7 processor and 4 physical cores.
%The processor supports hyperthreading, bringing the number of logical cores to 8.
For the main evaluation, TriOptimaâ€™s acceptance test environment with hardware similar to what is used in production, was used. This is a stationary computer running Linux CentOS,
with a dual processor architecture. The processors are 2 Intel Xeon E5504 processors with 4 cores each, making a total of 8 cores. The cores do not support hyperthreading. The machine
has 32 GB of memory.

\section{Test datasets}
For all datasets, the number of filters is limited to a few more than the number of columns, as the majority of the filters are column mapping filters.
This is typical for most datasets, making this a suitable sample of datasets. No datasets with inherently serial file formats where used, as these cannot be
parallelized and would provide no interesting data. Instead, the focus of the experiments are on the \textit{Extra overhead} file formats, since these are the
most common (since making trade ID:s unique is a useful feature). They also give the fairest indication of how successful the parallelization is, as they are
both the worst case (apart from inherently serial file formats) and the common case. Another reason why this set of datasets was chosen is their differing sizes,
with potentially differing parallelization benefits.

The characteristics of the test datasets are outlined in figure \ref{fig:test_datasets}.

%\subsection{Large}
%\begin{itemize}
  %\item \textbf{Rows:} 338,730
  %\item \textbf{Columns:} 89
  %\item \textbf{File format family:} Extra overhead
%\end{itemize}
\begin{table}[ht]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
Dataset & Rows   & Columns & File format family \\ \hline
Tiny    & 102    & 46      & Extra overhead     \\ \hline
Small   & 2,890  & 46      & Extra overhead     \\ \hline
Medium  & 23,763 & 46      & Extra overhead     \\ \hline
Large  & 349309 & 41      & Extra overhead     \\ \hline
\end{tabular}
\label{fig:test_datasets}
\caption{Test datasets.}
\end{table}

\section{Experiments}
\subsection{Benchmarks}
For each dataset, experiments were run to gather information about time and memory consumption for different configurations.
The experiments entailed performing a dataset transformation using the parallel program with each number of workers in the range 1 to 2 times the number of cores: 1--16.
In addition to running the parallel program, a serial transformation was also performed as a reference point.
In each of the experiments, the Python \code{resource} module was used in each worker process to find the user time, system time, and maximum memory consumption.
These values were then summed in order to find the total resource use. In the case of maximum memory consumption, this means that the number found is a worst case
number. It is possible that the processes in total have a smaller maximum memory consumption since they may reach their maximum memory consumption at different
points in time.

In order to provide more accurate results, the experiments were run 10 times for each dataset--worker number configuration. These 10 runs were then used to calculate the
average value as well as the standard deviation for each metric value. In addition, the speedup was calculated by dividing the sequential execution time with the parallel execution
time for each worker value.

\subsection{Profiling}
For each dataset, a single run with 8 workers was profiled using \code{cProfile} in order to find where in the program the most time is spent. This was done in order
to better understand the benchmark results.
